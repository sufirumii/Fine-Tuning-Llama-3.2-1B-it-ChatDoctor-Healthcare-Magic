# LlamaTron RS1 ThinkDoc

A specialized medical AI assistant developed through supervised fine-tuning of Llama 3.2 1B Instruct on real doctor-patient conversations.

## Overview

LlamaTron RS1 ThinkDoc represents the first model in a progressive fine-tuning series, beginning with 100K-range datasets and scaling toward multi-million sample training in future iterations. This model demonstrates human-aligned conversational capabilities in medical consultation contexts.

## Model Details

### Base Model
- **Architecture**: Llama 3.2 1B Instruct (meta-llama/Llama-3.2-1B-Instruct)
- **Parameters**: 1.24 billion total parameters
- **Developer**: Meta AI

### Fine-Tuning Specifications
- **Method**: LoRA (Low-Rank Adaptation)
- **LoRA Rank**: 16
- **LoRA Alpha**: 32
- **Target Modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **Trainable Parameters**: 11.2M (0.90% of total)
- **Dropout**: 0.05

## Dataset

- **Name**: ChatDoctor-HealthCareMagic-100k
- **Source**: [https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k)
- **Size**: 112,165 medical conversation samples
- **Structure**: Instruction-input-output format containing real doctor-patient interactions
- **Domain**: General medical consultation across various specialties

## Training Configuration

### Hardware
- **GPU**: NVIDIA H200
- **Training Time**: Approximately 3 hours

### Hyperparameters
- **Epochs**: 2
- **Batch Size per Device**: 4
- **Gradient Accumulation Steps**: 4
- **Effective Batch Size**: 16
- **Learning Rate**: 3e-4
- **Learning Rate Scheduler**: Cosine with 3% warmup
- **Optimizer**: Paged AdamW 8-bit
- **Max Sequence Length**: 1024 tokens
- **Precision**: BFloat16

### Data Split
- **Training**: 106,548 samples (95%)
- **Evaluation**: 5,608 samples (5%)

## Model Capabilities

The model is designed to:
- Generate medically-informed responses to patient queries
- Maintain professional and empathetic communication style
- Provide structured medical advice including symptom analysis and recommendations
- Mirror the consultation patterns of practicing physicians

## Limitations

- Trained exclusively on text-based medical conversations
- Should not replace professional medical advice
- Limited to patterns observed in training data
- May generate plausible but incorrect medical information
- Not evaluated on clinical benchmarks

## Technical Stack

- **Framework**: Hugging Face Transformers
- **Fine-tuning Library**: PEFT (Parameter-Efficient Fine-Tuning)
- **Training Library**: Hugging Face Trainer
- **Model Format**: SafeTensors

## File Structure
```
LlamaTron-RS1-ThinkDoc/
├── Dataset/
│   └── medical_chat_formatted.jsonl
├── Fine Tuned Files/
│   ├── final_model/          # LoRA adapter weights
│   └── merged_model/          # Full merged model
└── scripts/
    ├── dataset_preparation.py
    ├── training.py
    └── inference_interface.py
```

## Usage

### Loading the Model
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

MODEL_PATH = "path/to/merged_model"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

### Inference
```python
messages = [
    {"role": "system", "content": "If you are a doctor, please answer the medical questions based on the patient's description."},
    {"role": "user", "content": "I have a severe headache and fever for 3 days. What should I do?"}
]

text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
input_ids = tokenizer(text, return_tensors="pt").input_ids.to(model.device)

with torch.no_grad():
    output = model.generate(
        input_ids,
        max_new_tokens=400,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

response = tokenizer.decode(output[0][input_ids.shape[-1]:], skip_special_tokens=True)
print(response)
```

## Future Work

### RS2 Development
- Integration of 800K+ chain-of-thought reasoning samples
- Extended context length support
- Multi-turn conversation enhancement

### Long-term Roadmap
- Scaling to multi-million sample datasets
- Clinical benchmark evaluation
- Multilingual medical consultation support
- Specialized domain adaptations

## Ethical Considerations

This model is intended for research and educational purposes. Medical advice generated by this AI should not be considered a substitute for professional medical consultation. Users should always consult qualified healthcare providers for medical decisions.

## Acknowledgments

- Meta AI for the Llama 3.2 base model
- Lavita for the ChatDoctor-HealthCareMagic-100k dataset
- Hugging Face for the transformers and PEFT libraries

## License

This project uses the Llama 3.2 model which is subject to Meta's license terms. The fine-tuned weights are released under the same license. Please refer to the original Llama 3.2 license for usage terms.

## Contact

For questions or collaboration inquiries, please open an issue in this repository.

---

**Version**: RS1 (Research Series 1)  
**Status**: Completed  
**Last Updated**: February 2026
